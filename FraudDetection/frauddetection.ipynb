{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14242,"databundleVersionId":568274,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-19T23:00:14.582182Z","iopub.execute_input":"2025-10-19T23:00:14.582642Z","iopub.status.idle":"2025-10-19T23:00:16.978882Z","shell.execute_reply.started":"2025-10-19T23:00:14.582607Z","shell.execute_reply":"2025-10-19T23:00:16.977802Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/ieee-fraud-detection/sample_submission.csv\n/kaggle/input/ieee-fraud-detection/test_identity.csv\n/kaggle/input/ieee-fraud-detection/train_identity.csv\n/kaggle/input/ieee-fraud-detection/test_transaction.csv\n/kaggle/input/ieee-fraud-detection/train_transaction.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport gc\nimport math\nimport json\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import KFold\n\nimport lightgbm as lgb\n\n\n# -------------------------------\n# Config\n# -------------------------------\nDATA_DIR = \"/kaggle/input/ieee-fraud-detection\"\nTRAIN_TRANS = f\"{DATA_DIR}/train_transaction.csv\"\nTEST_TRANS = f\"{DATA_DIR}/test_transaction.csv\"\nTRAIN_ID = f\"{DATA_DIR}/train_identity.csv\"\nTEST_ID = f\"{DATA_DIR}/test_identity.csv\"\nSAMPLE_SUB = f\"{DATA_DIR}/sample_submission.csv\"\n\n\nSEED = 42\nN_FOLDS = 5 # time‑ordered folds\nPURGE_DAYS = 1 # optional gap (in days) between train and valid to reduce leakage\nnp.random.seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T23:00:16.980009Z","iopub.execute_input":"2025-10-19T23:00:16.980451Z","iopub.status.idle":"2025-10-19T23:00:25.059069Z","shell.execute_reply.started":"2025-10-19T23:00:16.980427Z","shell.execute_reply":"2025-10-19T23:00:25.058080Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# -------------------------------\n# Utilities\n# -------------------------------\n\ndef reduce_mem_usage(df: pd.DataFrame, verbose=True):\n    start_mem = df.memory_usage(deep=True).sum() / 1024**2\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type != object and str(col_type) != 'category':\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min >= np.iinfo(np.int8).min and c_max <= np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min >= np.iinfo(np.int16).min and c_max <= np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min >= np.iinfo(np.int32).min and c_max <= np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                else:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                df[col] = df[col].astype(np.float32)\n    # leave objects as is\n    end_mem = df.memory_usage(deep=True).sum() / 1024**2\n    if verbose:\n        print(f\"Mem. usage decreased to {end_mem:5.2f} Mb ({100*(start_mem-end_mem)/start_mem:.1f}% reduction)\")\n    return df\n\n\n# Basic cleaners for messy strings\nEMAIL_MAP = {\n'gmail.com': 'gmail', 'gmail': 'gmail',\n'yahoo.com': 'yahoo', 'yahoo': 'yahoo',\n'hotmail.com': 'hotmail', 'outlook.com': 'outlook', 'live.com': 'microsoft',\n'aol.com': 'aol', 'icloud.com': 'apple', 'me.com': 'apple', 'mac.com': 'apple'\n}\n\ndef clean_email_domain(s: pd.Series) -> pd.Series:\n    s = s.fillna('')\n    s = s.str.lower()\n    s = s.replace(EMAIL_MAP)\n    s = s.where(s != '', np.nan)\n    return s\n\n\ndef split_device_info(dev_series: pd.Series):\n    # DeviceInfo examples like \"Windows; Chrome 70\" or vendor/version strings\n    base = dev_series.fillna('').str.lower()\n    brand = base.str.extract(r'^([a-z0-9_\\-\\s]+)')[0].str.strip()\n    # Keep only first token as brand proxy\n    brand = brand.str.split(' ').str[0]\n    brand = brand.replace({'samsungsm': 'samsung', 'samsung': 'samsung', 'windows': 'windows',\n    'iphone': 'iphone', 'ipad': 'ipad', 'mac': 'mac', 'huawei': 'huawei'})\n    brand = brand.where(brand != '', np.nan)\n    return brand\n\n\ndef make_uid(df: pd.DataFrame):\n    # Stable-ish user key; you can try richer combos later\n    uid = df['card1'].astype('float32').astype('Int32').astype('string') + '_' + df['addr1'].astype('float32').astype('Int32').astype('string')\n    uid = uid.replace({'<NA>_': np.nan, '_<NA>': np.nan, '<NA>_<NA>': np.nan})\n    return uid\n\n\ndef add_time_features(df: pd.DataFrame):\n    # TransactionDT is seconds from a reference point; derive day/hour\n    df['DT_day'] = (df['TransactionDT'] / (24*60*60)).astype(np.int32)\n    df['DT_hour'] = (df['TransactionDT'] / (60*60)).astype(np.int32)\n    df['DT_dayofweek'] = (df['DT_day'] % 7).astype(np.int8)\n    df['DT_week'] = (df['DT_day'] // 7).astype(np.int32)\n    return df\n\n\ndef frequency_encode(train: pd.DataFrame, test: pd.DataFrame, cols):\n    for c in cols:\n        freq = train[c].value_counts(dropna=False)\n        train[f'{c}_freq'] = train[c].map(freq).astype(np.float32)\n        test[f'{c}_freq'] = test[c].map(freq).astype(np.float32)\n    return train, test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T23:00:25.061293Z","iopub.execute_input":"2025-10-19T23:00:25.061972Z","iopub.status.idle":"2025-10-19T23:00:25.080143Z","shell.execute_reply.started":"2025-10-19T23:00:25.061947Z","shell.execute_reply":"2025-10-19T23:00:25.079067Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# -------------------------------\n# Read & Merge\n# -------------------------------\nprint(\"Reading data…\")\ntrain_tr = pd.read_csv(TRAIN_TRANS)\ntrain_id = pd.read_csv(TRAIN_ID)\n\ntest_tr = pd.read_csv(TEST_TRANS)\ntest_id = pd.read_csv(TEST_ID)\n\nprint(\"Merging identity…\")\ntrain = train_tr.merge(train_id, how='left', on='TransactionID')\ntrain.drop(columns=['TransactionID'], inplace=True)\n\ntest = test_tr.merge(test_id, how='left', on='TransactionID')\n# Keep for submission later\ntest_transaction_ids = test_tr['TransactionID'].values\n\n# Reduce mem a bit before feature work\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)\n\ndel train_tr, train_id, test_tr, test_id\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T23:00:25.081287Z","iopub.execute_input":"2025-10-19T23:00:25.081560Z","iopub.status.idle":"2025-10-19T23:01:46.169998Z","shell.execute_reply.started":"2025-10-19T23:00:25.081539Z","shell.execute_reply":"2025-10-19T23:01:46.168826Z"}},"outputs":[{"name":"stdout","text":"Reading data…\nMerging identity…\nMem. usage decreased to 1654.18 Mb (35.4% reduction)\nMem. usage decreased to 1436.05 Mb (35.1% reduction)\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# -------------------------------\n# Light Cleaning & Feature Engineering (SAFE)\n# -------------------------------\nprint(\"Feature engineering…\")\n\nimport re\n\ndef normalize_id_columns(df):\n    \"\"\"Rename id-* / id_* columns to a unified id_XX format.\"\"\"\n    mapping = {}\n    for c in df.columns:\n        m = re.match(r'^id[-_](\\d{2})$', c)\n        if m:\n            mapping[c] = f'id_{m.group(1)}'\n    if mapping:\n        df.rename(columns=mapping, inplace=True)\n\n# normalize both train and test once\nnormalize_id_columns(train)\nnormalize_id_columns(test)\n\n\n# Helper: present columns\ndef present_cols(df, cols):\n    return [c for c in cols if c in df.columns]\n\nALL_ID_COLS = [f'id_{i:02d}' for i in list(range(1,21)) + [28,29,30,31,32,33,34,35,36,37,38]]\nID_TRAIN = present_cols(train, ALL_ID_COLS)\nID_TEST  = present_cols(test,  ALL_ID_COLS)\n\n\n# --- A) De-categorize columns we will mutate (avoid 'new category' errors) ---\ncols_we_mutate = [\n    'P_emaildomain','R_emaildomain','DeviceInfo','DeviceType','DeviceBrand',\n    'card4','card6','M1','M2','M3','M4','M5','M6','M7','M8','M9'\n]\nfor c in cols_we_mutate:\n    if c in train.columns: train[c] = train[c].astype('object')\n    if c in test.columns:  test[c]  = test[c].astype('object')\n\n# --- B) Email + Device cleanup ---\nfor col in ['P_emaildomain', 'R_emaildomain']:\n    if col in train.columns: train[col] = clean_email_domain(train[col])\n    if col in test.columns:  test[col]  = clean_email_domain(test[col])\n\nif 'DeviceInfo' in train.columns:\n    train['DeviceBrand'] = split_device_info(train['DeviceInfo'])\nif 'DeviceInfo' in test.columns:\n    test['DeviceBrand'] = split_device_info(test['DeviceInfo'])\n\n# --- C) Time features from TransactionDT ---\ntrain = add_time_features(train)\ntest  = add_time_features(test)\n\n# --- D) Stable user key ---\ntrain['uid'] = make_uid(train)\ntest['uid']  = make_uid(test)\n\n# --- E) Numeric transforms ---\nfor df in [train, test]:\n    if 'TransactionAmt' in df.columns:\n        df['TransactionAmt_log1p'] = np.log1p(df['TransactionAmt'])\n\n# --- F) Cast common string columns to 'category' (AFTER cleaning) ---\nmaybe_cats = [\n    'ProductCD','P_emaildomain','R_emaildomain','DeviceType','DeviceInfo','DeviceBrand',\n    'card4','card6','M1','M2','M3','M4','M5','M6','M7','M8','M9','uid'\n]\npresent_cats_train = present_cols(train, maybe_cats) + ID_TRAIN\npresent_cats_test  = present_cols(test,  maybe_cats) + ID_TEST\n\nfor c in present_cats_train: train[c] = train[c].astype('category')\nfor c in present_cats_test:  test[c]  = test[c].astype('category')\n\n# --- G) Frequency encodings (fit on train only; safe intersections) ---\nfreq_candidates = ['uid','card1','card2','addr1','addr2',\n                   'P_emaildomain','R_emaildomain','DeviceBrand','DeviceType','ProductCD']\nfreq_cols = present_cols(train, freq_candidates)\ntrain, test = frequency_encode(train, test, freq_cols)\n\n# --- H) Build feature list & prune ultra-sparse columns ---\nTARGET = 'isFraud'\nignore_cols = [TARGET]\nfeature_cols = [c for c in train.columns if c not in ignore_cols]\n\nna_rate = train[feature_cols].isna().mean()\nfeature_cols = na_rate[na_rate < 0.98].index.tolist()\n\n\n# --- I) Ensure LightGBM-compatible dtypes ---\n# Drop any remaining object/string columns (keep their *_freq encodings instead)\nobj_like = train[feature_cols].select_dtypes(include=['object','string']).columns.tolist()\nfeature_cols = [c for c in feature_cols if c not in obj_like]\n\n# Track categoricals that are actually used\ncategorical_cols = [c for c in feature_cols if str(train[c].dtype).startswith('category')]\n\n# --- J) Quick sanity prints ---\nprint(\"Dropped non-numeric columns:\", obj_like)\nprint(\"#Features used:\", len(feature_cols))\nprint(\"Sample features:\", feature_cols[:20])\nprint(\"Categorical cols:\", categorical_cols[:20])\nprint(\"Present id_* in TRAIN:\", ID_TRAIN[:10], \"…\", len(ID_TRAIN))\nprint(\"Present id_* in TEST :\", ID_TEST[:10],  \"…\", len(ID_TEST))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T23:01:46.171155Z","iopub.execute_input":"2025-10-19T23:01:46.171390Z","iopub.status.idle":"2025-10-19T23:02:00.850813Z","shell.execute_reply.started":"2025-10-19T23:01:46.171371Z","shell.execute_reply":"2025-10-19T23:02:00.849750Z"}},"outputs":[{"name":"stdout","text":"Feature engineering…\nDropped non-numeric columns: []\n#Features used: 440\nSample features: ['TransactionDT', 'TransactionAmt', 'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1', 'dist2', 'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C3', 'C4', 'C5']\nCategorical cols: ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06']\nPresent id_* in TRAIN: ['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10'] … 31\nPresent id_* in TEST : ['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10'] … 31\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# -------------------------------\n# Time‑Aware Purged CV Splitter\n# -------------------------------\nprint(\"Building time‑aware folds…\")\n\nif 'DT_day' not in train.columns:\n    raise RuntimeError('DT_day not found – ensure add_time_features() ran.')\n\n# We create folds by day quantiles so validation marches forward in time.\nday_values = train['DT_day']\nq = np.quantile(day_values, np.linspace(0, 1, N_FOLDS+1))\nfold_bounds = [(int(q[i]), int(q[i+1]) if i < N_FOLDS-1 else int(q[i+1])+1) for i in range(N_FOLDS)]\n# fold_bounds: list of (start_day, end_day_exclusive)\n\nfolds = []\nfor i, (d0, d1) in enumerate(fold_bounds):\n    # Purge: exclude days immediately before/after validation from training\n    train_mask = (train['DT_day'] < (d0 - PURGE_DAYS))\n    valid_mask = (train['DT_day'] >= d0) & (train['DT_day'] < d1)\n    # Also ensure no future leakage: training strictly before validation window\n    train_mask &= (train['DT_day'] < d0)\n\n    tr_idx = np.where(train_mask)[0]\n    va_idx = np.where(valid_mask)[0]\n    if len(va_idx) == 0 or len(tr_idx) == 0:\n        continue\n    folds.append((tr_idx, va_idx))\n\nprint(f\"Constructed {len(folds)} folds with purge.\")\n\n# -------------------------------\n# Train LightGBM with Early Stopping\n# -------------------------------\nparams = {\n    'objective': 'binary',\n    'metric': 'auc',\n    'boosting_type': 'gbdt',\n    'learning_rate': 0.05,\n    'num_leaves': 256,\n    'max_depth': -1,\n    'min_data_in_leaf': 200,\n    'feature_fraction': 0.7,\n    'bagging_fraction': 0.7,\n    'bagging_freq': 1,\n    'lambda_l1': 0.0,\n    'lambda_l2': 0.0,\n    'min_gain_to_split': 0.0,\n    'is_unbalance': True,  # class imbalance helper\n    'verbosity': -1,\n    'seed': SEED,\n    'num_threads': max(1, os.cpu_count()-1),\n}\n\nprint(\"Training…\")\n\noof_pred = np.zeros(len(train), dtype=np.float32)\ntest_pred = np.zeros(len(test), dtype=np.float32)\nfeature_importance = pd.DataFrame({'feature': feature_cols, 'importance': 0})\n\nfor fold, (tr_idx, va_idx) in enumerate(folds, 1):\n    print(f\"\\nFold {fold}/{len(folds)}: train={len(tr_idx)}, valid={len(va_idx)}\")\n    tr_data = lgb.Dataset(train.loc[tr_idx, feature_cols], label=train.loc[tr_idx, TARGET])\n    va_data = lgb.Dataset(train.loc[va_idx, feature_cols], label=train.loc[va_idx, TARGET])\n\n    model = lgb.train(\n    params,\n    tr_data,\n    num_boost_round=10000,\n    valid_sets=[tr_data, va_data],\n    valid_names=['train','valid'],\n    callbacks=[\n        lgb.early_stopping(stopping_rounds=300),\n        lgb.log_evaluation(period=200)\n    ],\n)\n\n    oof_pred[va_idx] = model.predict(train.loc[va_idx, feature_cols], num_iteration=model.best_iteration)\n    test_pred += model.predict(test[feature_cols], num_iteration=model.best_iteration) / len(folds)\n\n    # Importance\n    fi = pd.DataFrame({  \n        'feature': feature_cols,\n        'importance': model.feature_importance(importance_type='gain')\n    })\n    feature_importance['importance'] += fi['importance'].values\n\n    del tr_data, va_data, model\n    gc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T23:02:00.851672Z","iopub.execute_input":"2025-10-19T23:02:00.851954Z","iopub.status.idle":"2025-10-19T23:56:54.947652Z","shell.execute_reply.started":"2025-10-19T23:02:00.851931Z","shell.execute_reply":"2025-10-19T23:56:54.946505Z"}},"outputs":[{"name":"stdout","text":"Building time‑aware folds…\nConstructed 4 folds with purge.\nTraining…\n\nFold 1/4: train=110549, valid=117590\nTraining until validation scores don't improve for 300 rounds\n[200]\ttrain's auc: 1\tvalid's auc: 0.861782\n[400]\ttrain's auc: 1\tvalid's auc: 0.868853\n[600]\ttrain's auc: 1\tvalid's auc: 0.871612\n[800]\ttrain's auc: 1\tvalid's auc: 0.86979\nEarly stopping, best iteration is:\n[565]\ttrain's auc: 1\tvalid's auc: 0.871737\n\nFold 2/4: train=229906, valid=119835\nTraining until validation scores don't improve for 300 rounds\n[200]\ttrain's auc: 0.999985\tvalid's auc: 0.879576\n[400]\ttrain's auc: 1\tvalid's auc: 0.886842\n[600]\ttrain's auc: 1\tvalid's auc: 0.893049\n[800]\ttrain's auc: 1\tvalid's auc: 0.89677\n[1000]\ttrain's auc: 1\tvalid's auc: 0.898277\n[1200]\ttrain's auc: 1\tvalid's auc: 0.899389\n[1400]\ttrain's auc: 1\tvalid's auc: 0.900028\n[1600]\ttrain's auc: 1\tvalid's auc: 0.899613\nEarly stopping, best iteration is:\n[1462]\ttrain's auc: 1\tvalid's auc: 0.900195\n\nFold 3/4: train=350664, valid=118290\nTraining until validation scores don't improve for 300 rounds\n[200]\ttrain's auc: 0.999918\tvalid's auc: 0.909225\n[400]\ttrain's auc: 0.999999\tvalid's auc: 0.914823\n[600]\ttrain's auc: 1\tvalid's auc: 0.918137\n[800]\ttrain's auc: 1\tvalid's auc: 0.921336\n[1000]\ttrain's auc: 1\tvalid's auc: 0.92288\n[1200]\ttrain's auc: 1\tvalid's auc: 0.922867\n[1400]\ttrain's auc: 1\tvalid's auc: 0.92318\n[1600]\ttrain's auc: 1\tvalid's auc: 0.922637\nEarly stopping, best iteration is:\n[1371]\ttrain's auc: 1\tvalid's auc: 0.923601\n\nFold 4/4: train=469377, valid=118534\nTraining until validation scores don't improve for 300 rounds\n[200]\ttrain's auc: 0.999787\tvalid's auc: 0.903826\n[400]\ttrain's auc: 0.999995\tvalid's auc: 0.908279\n[600]\ttrain's auc: 1\tvalid's auc: 0.911805\n[800]\ttrain's auc: 1\tvalid's auc: 0.915\n[1000]\ttrain's auc: 1\tvalid's auc: 0.917385\n[1200]\ttrain's auc: 1\tvalid's auc: 0.918969\n[1400]\ttrain's auc: 1\tvalid's auc: 0.919727\n[1600]\ttrain's auc: 1\tvalid's auc: 0.919919\n[1800]\ttrain's auc: 1\tvalid's auc: 0.920355\n[2000]\ttrain's auc: 1\tvalid's auc: 0.920303\n[2200]\ttrain's auc: 1\tvalid's auc: 0.92086\n[2400]\ttrain's auc: 1\tvalid's auc: 0.920852\n[2600]\ttrain's auc: 1\tvalid's auc: 0.92014\nEarly stopping, best iteration is:\n[2342]\ttrain's auc: 1\tvalid's auc: 0.920981\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------\n# Evaluation & Submission\n# -------------------------------\noof_auc = roc_auc_score(train[TARGET], oof_pred)\nprint(f\"\\nOOF AUC: {oof_auc:.6f}\")\n\n\n# Save feature importance\nfeature_importance.sort_values('importance', ascending=False, inplace=True)\nfeature_importance.to_csv('feature_importance.csv', index=False)\nprint(\"Saved feature_importance.csv\")\n\n\n# Submission\nsub = pd.read_csv(SAMPLE_SUB)\nsub['isFraud'] = test_pred\nsub.to_csv('submission.csv', index=False)\nprint(\"Saved submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-19T23:56:54.949483Z","iopub.execute_input":"2025-10-19T23:56:54.949790Z","iopub.status.idle":"2025-10-19T23:56:56.303015Z","shell.execute_reply.started":"2025-10-19T23:56:54.949760Z","shell.execute_reply":"2025-10-19T23:56:56.301816Z"}},"outputs":[{"name":"stdout","text":"\nOOF AUC: 0.811578\nSaved feature_importance.csv\nSaved submission.csv\n","output_type":"stream"}],"execution_count":7}]}